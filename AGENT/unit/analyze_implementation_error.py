# ä¿®æ”¹SUBMISSION_LANGUAGEä½¿ä¹‹ä½œä¸ºå‚æ•°ä¼ é€’

import uuid
import re
import traceback
from coper.LLM import LLM
from coper.basic_ops import Mul
from core.Context import Context
from coper.Service import Service
import logging
from AGENT.unit.pydantic_models import ImplementationAnalysisOutput
log_filename = f"judge_error_type.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)
SUBMISSION_LANGUAGE = "C++14"
def analyze_implementation_error(problem_desc: str, student_code: str, error_analysis: dict,llm_model: str) -> str:
    """
    å½“ LLM åˆ¤æ–­ä¸ºå®ç°æ€§é”™è¯¯æ—¶ï¼Œä½¿ç”¨ LLM åˆ†æå…·ä½“çš„å®ç°é—®é¢˜å¹¶æä¾›ä¿®å¤å»ºè®®ã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«åˆ†æå’Œå»ºè®®çš„å­—ç¬¦ä¸²ã€‚
    """
    logging.info("ğŸ¤– LLM: æ­£åœ¨åˆ†æå®ç°æ€§é”™è¯¯å¹¶æä¾›ä¿®å¤å»ºè®®...")
    base_prompt = f"""
[SYSTEM]
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ç«èµ›ç¼–ç¨‹å¯¼å¸ˆã€‚å­¦ç”Ÿçš„ä»£ç å› å®ç°æ€§é”™è¯¯è€Œè¢«åˆ¤é”™è¯¯ï¼Œä½†å…¶æ ¸å¿ƒé€»è¾‘æ˜¯å¥å…¨çš„ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ç²¾ç¡®åœ°æŒ‡å‡ºä»£ç ä¸­å­˜åœ¨é”™è¯¯çš„å…·ä½“å®ç°ç»†èŠ‚ï¼Œå¹¶æä¾›æ¸…æ™°ã€å¯æ“ä½œçš„ä¿®å¤å»ºè®®ã€‚

[é—®é¢˜é™ˆè¿°]
{problem_desc}

[å­¦ç”Ÿä»£ç ]
```{SUBMISSION_LANGUAGE.lower()}
{student_code}
[LLM å¯¹å®ç°æ€§é”™è¯¯çš„åˆæ­¥åˆ†æ]
åŸå› ï¼š{error_analysis.get('reasoning', 'æœªæä¾›åŸå› ã€‚')}
"""
    structured_prompt = base_prompt + """
analysis: ä»”ç»†æ£€æŸ¥å­¦ç”Ÿä»£ç ï¼Œæ‰¾å‡ºåŒ…å« Bug çš„ç¡®åˆ‡ä»£ç è¡Œæˆ–ä»£ç æ®µï¼Œå¹¶è¯¦ç»†è§£é‡Šä¸ºä»€ä¹ˆå®ƒé”™äº†ã€‚
suggestion: æä¾›ä¿®æ­£åçš„ä»£ç ç‰‡æ®µæˆ–æ¸…æ™°çš„ä¿®å¤è¯´æ˜ã€‚å¦‚æœå­˜åœ¨å¤šä¸ªé—®é¢˜ï¼Œè¯·ä¸€å¹¶è¯´æ˜ã€‚
"""
    natural_prompt = base_prompt + """
[ä»»åŠ¡]
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä½ çš„åˆ†æå’Œå»ºè®®ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæ€§æ–‡å­—ï¼š
å®ç°é”™è¯¯åˆ†æ:
<åœ¨è¿™é‡Œå¯¹é”™è¯¯çš„è¯¦ç»†åˆ†æ>
å»ºè®®ä¿®å¤:
<åœ¨è¿™é‡Œæä¾›ä¿®æ­£åçš„ä»£ç ç‰‡æ®µæˆ–å…·ä½“è¯´æ˜>
"""

    try:
        llm = LLM(model=llm_model)
        analysis = None
        suggestion = None

        # === ä¸»æ¨¡å¼ï¼šå°è¯•ç»“æ„åŒ–è¾“å‡º ===
        try:
            response = llm(structured_prompt, structured_output=ImplementationAnalysisOutput.model_json_schema()).result()
            structured_data = response.get("structured_output")

            if structured_data and "analysis" in structured_data and "suggestion" in structured_data:
                analysis = structured_data.get("analysis")
                suggestion = structured_data.get("suggestion")
                logging.info("âœ… AI å·²é€šè¿‡ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆåˆ†æå’Œå»ºè®®ã€‚")
        
        except Exception as e:
            logging.warning(f"ç»“æ„åŒ–è¾“å‡ºå¤±è´¥: {e}, å°†å°è¯•è‡ªç„¶è¯­è¨€è§£ææ–¹å¼...")

        # === å¤‡ç”¨æ¨¡å¼ï¼šå¦‚æœä¸»æ¨¡å¼å¤±è´¥ï¼Œåˆ™å°è¯•è‡ªç„¶è¯­è¨€è§£æ ===
        if analysis is None or suggestion is None:
            response = llm(natural_prompt).result()
            content = response.get("content", "")
            logging.debug(f"AI è‡ªç„¶è¯­è¨€å“åº”å†…å®¹:\n{content}")

            analysis_match = re.search(r"å®ç°é”™è¯¯åˆ†æ:\s*\n?(.*?)å»ºè®®ä¿®å¤:", content, re.DOTALL | re.IGNORECASE)
            fix_match = re.search(r"å»ºè®®ä¿®å¤:\s*\n?(.*)", content, re.DOTALL | re.IGNORECASE)
            
            analysis = analysis_match.group(1).strip() if analysis_match else "LLM æœªæä¾›å…·ä½“åˆ†æã€‚"
            suggestion = fix_match.group(1).strip() if fix_match else "LLM æœªæä¾›å…·ä½“ä¿®å¤å»ºè®®ã€‚"

            if analysis_match and fix_match:
                logging.info("âœ… AI å·²æŒ‰è‡ªç„¶è¯­è¨€æ ¼å¼è¿”å›åˆ†æå’Œå»ºè®®ã€‚")

        # --- æ„å»ºå¹¶è¿”å›æœ€ç»ˆç»“æœ ---
        result = {
            "error_type": "implementation",
            "reasoning": error_analysis.get('reasoning', 'æœªæä¾›åŸå› ã€‚'),
            "implementation_analysis": analysis,
            "fix_suggestions": suggestion
        }
        
        logging.info(f"å®ç°é”™è¯¯åˆ†æ:\n{analysis}\n\nå»ºè®®ä¿®å¤:\n{suggestion}")
        return result

    except Exception as e:
        logging.error(f"âŒ ä½¿ç”¨ LLM åˆ†æå®ç°é”™è¯¯æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        logging.error(traceback.format_exc())
        return {
            "error_type": "implementation",
            "reasoning": error_analysis.get('reasoning', 'Analysis failed'),
            "implementation_analysis": f"LLM analysis failed: {e}",
            "fix_suggestions": "Unable to provide suggestions due to an error."
    }